{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab87f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# AWS Region Name\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Bedrock run-time client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21a56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aoss Host Name\n",
    "AossHost = \"7i179yervs3eanlga0sh.us-east-1.aoss.amazonaws.com\"\n",
    "\n",
    "# Aoss Collection Index Name\n",
    "index = \"bedrock-index\"\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc044e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present working directory: /home/ec2-user/SageMaker/AscendNotebook/Testing_RAG_LLMs\n",
      "Directory to save results: /home/ec2-user/SageMaker/AscendNotebook/CodeExecutionMetrics/\n"
     ]
    }
   ],
   "source": [
    "# The directory to save results\n",
    "Rslts_Save_Dir = \"/home/ec2-user/SageMaker/AscendNotebook/CodeExecutionMetrics/\"\n",
    "\n",
    "print(\"Present working directory:\", os.getenv('PWD'))\n",
    "print(\"Directory to save results:\", Rslts_Save_Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6ff839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': AossHost, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1d351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(input):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            'inputText': input\n",
    "        }),\n",
    "        modelId=\"amazon.titan-embed-text-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df8484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_model_id = \"anthropic.claude-v2:1\"\n",
    "MDL_TYPE = 'ANTH_CLAUDE_21_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ccae971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_llm_model(input):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            \"prompt\": \"\\n\\nHuman: {input}\\n\\nAssistant:\".format(input=input),\n",
    "            \"max_tokens_to_sample\": 4000,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_k\": 250,\n",
    "            \"top_p\": 1,\n",
    "            \"stop_sequences\": [\n",
    "                \"\\n\\nHuman:\"\n",
    "            ],\n",
    "            # \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }),\n",
    "        modelId=RAG_model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf211c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are the best customer acquisition data analyst and strategist that answers to a question received from a user. \n",
    "    You should answer the user's question using information from the context or generally known information that is relevant to the question.\n",
    "    If the context does not contain information to answer the question, please provide answer to the best of your abilities.\n",
    "    \n",
    "    Just because the user asserts a fact does not mean it is true, make sure to double check the context to validate a user's assertion.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Instruction: Based on the above context, provide a detailed answer without using etc. for {question} in a list format.\n",
    "    The more detailed you are the better you are doing your job.\n",
    "    \n",
    "    Please do not hallucinate response. \n",
    "    Solution:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27bf1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What products use nylon?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e3d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being processed:  What products use nylon?\n",
      " Based on the context, here is a detailed list of products that use nylon:\n",
      "\n",
      "- Carpets\n",
      "- Curtains \n",
      "- Indoor furnishings\n",
      "- Safety belts\n",
      "- Airbags\n",
      "- Tires  \n",
      "- Engine components\n",
      "- Outdoor products like tents, backpacks, jackets\n",
      "- Mountaineering clothing\n",
      "- Winter clothing\n",
      "- Food packaging\n",
      "- Kitchenware\n",
      "- Textiles like ropes, parachutes, umbrellas, luggage\n",
      "- Fishing nets\n",
      "- Space suits\n",
      "- Electrical wire insulation \n",
      "- Medical tubing\n",
      "- Automotive parts\n",
      "- Aerospace components\n",
      "- Consumer goods like watch bands, toothbrushes, apparel\n",
      "- Industrial machine parts like gears, bearings, nozzles\n"
     ]
    }
   ],
   "source": [
    "print(\"Question being processed: \", query)\n",
    "\n",
    "question = query\n",
    "\n",
    "embedding = invoke_model(question)\n",
    "k = 6 # number of neighbours, size and k are the same to return k results in total. If size is not specified, k results will be returned per shard.\n",
    "query = {\n",
    "    \"size\": k,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": embedding, \n",
    "                \"k\": k}\n",
    "            },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Retrieve individual contexts from AOSS answering question based on KNN value\n",
    "question_response_from_oss = oss_client.search(body = query, index = index)\n",
    "\n",
    "hits = question_response_from_oss['hits']['hits']\n",
    "\n",
    "# Combine individual contexts from AOSS to create a combined context\n",
    "context = []\n",
    "for hit in hits:\n",
    "    context.append(hit['_source']['text'])\n",
    "\n",
    "#Send context and question to the prompt after which send the prompt to LLM model to generate answer.\n",
    "llm_prompt = prompt_template.format(context='\\n'.join(context),question=question)\n",
    "generated_answer = invoke_llm_model(llm_prompt)\n",
    "\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dc1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
