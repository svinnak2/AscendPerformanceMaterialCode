{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab87f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# The following libraries are imported for RAGAS and LANGFUSE Implementation\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06c4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Region Name\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Bedrock run-time client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name = region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21a56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aoss Host Name\n",
    "AossHost = \"7i179yervs3eanlga0sh.us-east-1.aoss.amazonaws.com\"\n",
    "\n",
    "# Aoss Collection Index Name\n",
    "index = \"bedrock-index\"\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc044e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present working directory: /home/ec2-user/SageMaker/AscendNotebook/Testing_RAG_LLMs\n",
      "Directory to save results: /home/ec2-user/SageMaker/AscendNotebook/CodeExecutionMetrics/\n"
     ]
    }
   ],
   "source": [
    "# The directory to save results\n",
    "Rslts_Save_Dir = \"/home/ec2-user/SageMaker/AscendNotebook/CodeExecutionMetrics/\"\n",
    "\n",
    "print(\"Present working directory:\", os.getenv('PWD'))\n",
    "print(\"Directory to save results:\", Rslts_Save_Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc36dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get keys for your project from https://cloud.langfuse.com\n",
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-f7b59b69-3122-4669-ac56-ac1a566027fb\" #replace it with your public key\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-e399949a-a9fd-4730-90d2-3281a0ed23e4\" #replace it with you secret key\n",
    "LANGFUSE_HOST=\"https://us.cloud.langfuse.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefc54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockLLM`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 3000\n",
    "}\n",
    "\n",
    "llm_for_text_generation = Bedrock(model_id=\"anthropic.claude-instant-v1\",\n",
    "                                     model_kwargs=model_kwargs_claude,\n",
    "                                     streaming=True,\n",
    "                                     client = bedrock_client,)\n",
    "\n",
    "\n",
    "llm_for_evaluation = Bedrock(model_id=\"anthropic.claude-v2:1\",\n",
    "                                model_kwargs=model_kwargs_claude,\n",
    "                                streaming=True,\n",
    "                                client = bedrock_client,)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6e6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "ragas_bedrock_model = LangchainLLM(llm_for_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db72094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set embeddings model for evaluating answer relevancy metric\n",
    "answer_relevancy.embeddings = bedrock_embeddings\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        harmfulness\n",
    "    ]\n",
    "\n",
    "for m in metrics:\n",
    "    m.__setattr__(\"llm\", ragas_bedrock_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898adf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check connection to Langfuse: True\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(public_key=LANGFUSE_PUBLIC_KEY,secret_key=LANGFUSE_SECRET_KEY, host = LANGFUSE_HOST)\n",
    "print(\"Check connection to Langfuse:\", langfuse.auth_check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdb34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_ragas(query, chunks, answer):\n",
    "    scores = {}\n",
    "    for m in metrics:\n",
    "        print(f\"calculating {m.name}\")\n",
    "        scores[m.name] = m.score_single(\n",
    "            {\"question\": query, \"contexts\": chunks, \"answer\": answer}\n",
    "        )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6ff839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': AossHost, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1d351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(input):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            'inputText': input\n",
    "        }),\n",
    "        modelId=\"amazon.titan-embed-text-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df8484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_model_id = \"anthropic.claude-v2:1\"\n",
    "MDL_TYPE = 'ANTH_CLAUDE_21_'\n",
    "RAG_MAX_TOKENS = 4000\n",
    "RAG_TEMP = 0.1\n",
    "RAG_TOP_K = 250\n",
    "RAG_TOP_P = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccae971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_llm_model(input, RAG_MAX_TOKENS, RAG_TEMP, RAG_TOP_K, RAG_TOP_P, RAG_model_id):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            \"prompt\": \"\\n\\nHuman: {input}\\n\\nAssistant:\".format(input=input),\n",
    "            \"max_tokens_to_sample\": RAG_MAX_TOKENS,\n",
    "            \"temperature\": RAG_TEMP,\n",
    "            \"top_k\": RAG_TOP_K,\n",
    "            \"top_p\": RAG_TOP_P,\n",
    "            \"stop_sequences\": [\n",
    "                \"\\n\\nHuman:\"\n",
    "            ],\n",
    "            # \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }),\n",
    "        modelId=RAG_model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf211c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are the best customer acquisition data analyst and strategist that answers to a question received from a user. \n",
    "    You should answer the user's question using information from the context or generally known information that is relevant to the question.\n",
    "    If the context does not contain information to answer the question, please provide answer to the best of your abilities.\n",
    "    \n",
    "    Just because the user asserts a fact does not mean it is true, make sure to double check the context to validate a user's assertion.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Instruction: Based on the above context, provide a detailed answer without using etc. for {question} in a list format.\n",
    "    The more detailed you are the better you are doing your job.\n",
    "    \n",
    "    Please do not hallucinate response. \n",
    "    Solution:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27bf1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What products use nylon?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e3d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being processed:  What products use nylon?\n",
      " Based on the context, here is a detailed list of products that use nylon:\n",
      "\n",
      "- Carpets\n",
      "- Curtains \n",
      "- Indoor furnishings\n",
      "- Safety belts\n",
      "- Airbags\n",
      "- Tires  \n",
      "- Engine components\n",
      "- Outdoor products like tents, backpacks, jackets\n",
      "- Mountaineering clothing\n",
      "- Winter clothing\n",
      "- Food packaging\n",
      "- Kitchenware\n",
      "- Textiles like ropes, parachutes, umbrellas, luggage\n",
      "- Fishing nets\n",
      "- Space suits\n",
      "- Electrical wire insulation \n",
      "- Medical tubing\n",
      "- Automotive parts\n",
      "- Aerospace components\n",
      "- Consumer goods like watch bands, toothbrushes, apparel\n",
      "- Industrial machine parts like gears, bearings, nozzles\n",
      "calculating faithfulness\n",
      "calculating answer_relevancy\n",
      "calculating harmfulness\n",
      "Answer Relevance of this question is: 0.8413583458941173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/asyncio/base_events.py\", line 1894, in _run_once\n",
      "    handle = self._ready.popleft()\n",
      "IndexError: pop from an empty deque\n"
     ]
    }
   ],
   "source": [
    "print(\"Question being processed: \", query)\n",
    "\n",
    "question = query\n",
    "k = 6 # number of neighbours, size and k are the same to return k results in total. If size is not specified, k results will be returned per shard.\n",
    "\n",
    "Knn_Value = \"Knn Value: \" + str(k)\n",
    "Max_Tokens = \"Max_Tokens: \" + str(RAG_MAX_TOKENS)\n",
    "Temperature_var = \"Temparature: \" + str(RAG_TEMP)\n",
    "TOP_P_var = \"Top_P: \" + str(RAG_TOP_P)\n",
    "TOP_K_var = 'Top_K: ' + str(RAG_TOP_K)\n",
    "\n",
    "trace = langfuse.trace(name=\"Aoss\", user_id=\"APM_AWS\", \n",
    "                       tags = [Knn_Value, Max_Tokens, Temperature_var, TOP_P_var, TOP_K_var])\n",
    "\n",
    "embedding = invoke_model(question)\n",
    "query = {\n",
    "    \"size\": k,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": embedding, \n",
    "                \"k\": k}\n",
    "            },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Retrieve individual contexts from AOSS answering question based on KNN value\n",
    "question_response_from_oss = oss_client.search(body = query, index = index)\n",
    "\n",
    "hits = question_response_from_oss['hits']['hits']\n",
    "\n",
    "# Combine individual contexts from AOSS to create a combined context\n",
    "context = []\n",
    "for hit in hits:\n",
    "    context.append(hit['_source']['text'])\n",
    "    \n",
    "trace.span(name=\"retrieval\", \n",
    "                   input={\"question\": question}, \n",
    "                   output={\"contexts\": context},\n",
    "            )\n",
    "\n",
    "#Send context and question to the prompt after which send the prompt to LLM model to generate answer.\n",
    "llm_prompt = prompt_template.format(context='\\n'.join(context),question=question)\n",
    "generated_answer = invoke_llm_model(llm_prompt, RAG_MAX_TOKENS, RAG_TEMP, RAG_TOP_K, RAG_TOP_P, RAG_model_id)\n",
    "\n",
    "trace.span(\n",
    "        name=\"generation\",\n",
    "        input={\"question\": question, \"contexts\": context},\n",
    "        output={\"answer\": generated_answer}\n",
    "    )\n",
    "\n",
    "print(generated_answer)\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = score_with_ragas(question, context, generated_answer)\n",
    "ragas_scores\n",
    "for m in metrics:\n",
    "    trace.score(name=m.name, value=ragas_scores[m.name])\n",
    "    \n",
    "Answer_Relevance = ragas_scores['answer_relevancy']\n",
    "print(f\"Answer Relevance of this question is: {Answer_Relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf06408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
